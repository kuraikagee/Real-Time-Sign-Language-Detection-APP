This project focuses on sign language detection using machine learning and deploying it in an 
Android app. using a custom dataset comprising personally captured images, the project involves 
labeling, annotating, and training a custom YOLOv5 model. The ultimate goal is to develop an 
Android app from scratch in Android Studio that can accurately recognize and interpret sign 
language gestures captured through the device's camera. The app will utilize the trained YOLOv5 
model for real-time gesture detection and provide text or visual feedback for each detected 
gesture. Through the integration of machine learning, dataset creation, model training, and 
Android app development, this project aims to bridge the communication gap between sign 
language users and non-users, facilitating inclusive and accessible communication.This project 
focuses on sign language detection using machine learning and deploying it in an 
Android app. using a custom dataset comprising personally captured images, the project involves 
labeling, annotating, and training a custom YOLOv5 model. The ultimate goal is to develop an 
Android app from scratch in Android Studio that can accurately recognize and interpret sign 
language gestures captured through the device's camera. The app will utilize the trained YOLOv5 
model for real-time gesture detection and provide text or visual feedback for each detected 
gesture. Through the integration of machine learning, dataset creation, model training, and 
Android app development, this project aims to bridge the communication gap between sign 
language users and non-users, facilitating inclusive and accessible communication.